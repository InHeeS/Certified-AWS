# s3 moving between storage classes
- 객체를 아카이브할 예정임을 알고 있다면 Glacier 계층이나 Deep Archive 계층으로 이동
- 수명 주기 규칙은 객체를 구성해 다른 스토리지 클래스로 전환하기 위한 전환 작업입니다 예를 들어 생성한지 60일 후에는 표준 클래스로 이동하고 6개월 후에는 아카이빙을 위해 Glacier로 이동할 수 있습니다
- 특정 접두사에 규칙을 지정하여 전체 버킷이나 버킷 내의 특정 경로에 적용되도록 할 수 있음
- 특정 객체 태그에 지정 가능 경리부에만 규칙을 적용 가능
- 지난번에 객체의 클래스를 전환하는 데 가장 적합한 일 수는 Amazon S3 분석을 이용하면 됩니다
  - 표준이나 표준 IA를 권장하며 One Zone-IA나 Glacier에서는 사용할 수 없습니다
- 썸네일은 자주 액세스하지 않고 쉽게 다시 만들 수 있기 때문에 One Zone-IA에 보관할 수 있습니다
- 그리고 수명 주기 설정을 사용해 60일 후 만료되도록 하거나 삭제할 수 있습니다
- 그 다음 최신 버전이 아닌 버전, 즉 최상위 버전이 아닌 버전을 표준 IA로 전환하는 규칙을 만들어 이 최신이 아닌 버전을 추후에 아카이브를 위해 Glacier Deep Archive로 전환할 수 있습니다

# s3 evnet notifications
- 이벤트는 객체가 복원 되거나 복제되는 것
- s3 에서 aws service (sns, sqs, lambda, event bridge) 로 보내기 위해서는 접근 정책이 필요합니다.
- 이벤트는 모두 S3 버킷으로 가서 모두 Amazon EventBridge로 갑니다, 어떤 이벤트인지 상관없이요
- EventBridge에서 이 규칙 덕분에 18 종류의 AWS 서비스를 목적지로 해서 이벤트를 보낼 수 있습니다
  - Step Functions나 Kinesis Streams, Firehose 등에 보낼 수 있죠

# s3 baseline performance, 최적화 방법
- 따라서 버킷의 접두사당 초당 3,500개의 넣기 및 5,500개의 가져오기 규칙을 이해하기 쉽습니다
- 멀티파트 업로드로 100mb 넘는 파일에는 멀티파트 업로드를 사용 (5gb 넘는 파일은 반드시)
  - 그래서 이것은 공공 인터넷의 사용량을 최소화하고 프라이빗 AWS 네트워크의 사용량을 최대화하기 때문에 전송 가속화라고 불립니다
- 효율적으로 읽는 방법은 s3 바이트가 있습니다. 범위 가져오기를 통해 병렬화
- 또는 파일의 일부를 가져올때 업로드 및 다운로드 속도를 높일 수 있습니다.

# s3 user-defined object metadata & s3 object tags
- 객체를 만들고 객체를 업로드할 때 메타데이터도 할당할 수 있습니다
- AWS 내의 특정 태그가 있는 특정 객체에 액세스를 부여할 수 있기 때문입니다
- Amazon S3에서 메타데이터와 태그를 검색할 수 없다는 점으로 메타데이터로 필터링하거나 태그로 필터링할 수 없습니다
- key -value 형태
- S3 버킷에서 검색을 사용하려면 DynamoDB와 같은 데이터베이스에 외부 인덱스를 구축해야 함
  - DynamoDB에서 검색을 수행하면 검색 결과는 Amazon S3의 객체로 추출됩니다
 
# s3 object encryption
- 서버 측 암호화
  - SSE-S3는 Amazon S3 Managed Keys로 암호화입니다
  - KMS 키로 암호화 키를 관리해서 암호화하는 SSE-KMS가 있습니다
  - 사용자 제공 키를 사용하는 SSE-C도 있습니다
- 클라이언트 측 암호화
  - 클라이언트 측에서 암호화한 다음 Amazon S3에 업로드 하는 거죠

- sse-3
  - AWS에서 관리하고 소유합니다, 여러분은 이 키에 접근할 수 없어요
  - 서버 측에서 객체를 암호화 합니다.
  - ![image](https://github.com/user-attachments/assets/90e633d5-eba8-4b0f-8259-6e1516514008)

- sse-kms
  - KMS 서비스를 사용해 자신의 키를 직접 관리하려는 겁니다
  - CloudTrail을 사용해서 사용을 감시할 수 있습니다
  - 사용하려면 이런 헤더가 필요합니다, "x-amz-server-side-encryption":"aws:kms"그러면 서버 측에서 객체가 암호화 됩니다
  - 다른 헤더로 객체를 업로드합니다 헤더에 우리가 사용할 KMS 키를 명시하죠 그러면 객체가 Amazon S3에 나타납니다
  - 그리고 여기에서 사용할 KMS 키는 AWS KMS에서 가지고 오는 겁니다 이 두 가지가 혼합되어 암호화가 됩니다 이 파일이 S3 버킷에 저장되는 거죠
  - S3 버킷에서 파일을 읽으려면 객체 자체에도 접근해야 하지만 이 객체를 암호화하는 데 사용한 KMS 키에도 접근해야 합니다
  - Amazon S3에 파일을 업로드하거나 다운로드 할 때 KMS 키를 활용해야 합니다
  - 그래서 처리량이 아주 많은 S3 버킷이 있다면 KMS 키를 사용하여 모든 것이 암호화되므로 다소 주의가 필요합니다
 
- sse-c
  - 키는 AWS 외부에서 관리되지만 그래도 서버 측 암호화에 포함됩니다
  - AWS에 키를 보내기 때문이죠
  - 하지만 Amazon S3는 여러분이 제공한 키를 절대 저장하지 않아요
  - 작동 방식은, 사용자가 키와 함께 파일을 업로드합니다
  - Amazon S3는 클라이언트가 제공한 키와 객체를 사용해서 암호화를 수행하고 파일을 암호화한 상태로 S3 버킷에 저장합니다
  - 그리고 물론 해당 파일을 읽기 위해서는 사용자가 다시 암호화에 사용한 키를 제공해야 합니다

- client-side encryption
  - s3 에 데이터를 전달하기 전에 클라이언트가 암호화

# encryption in transit (전송 중 암호화)
- 전송 중 암호화는 SSL 혹은 TLS라고 합니다
- SSE-C 유형의 메커니즘을 사용하는 경우 HTTPS 프로토콜 사용이 필수입니다
- 암호화를 강제하는 방법으로 버킷 정책을 사용합니다.

# s3 default encryption
- 기본 암호화인 ㅇsse-s3를 사용합니다.
- 이는 예시에 불과하지만 버킷 정책으로도 버킷의 암호화를 강제로 적용할 수 있다는 점을 확인 가능하다.
- 기본 암호화는 S3에서 기본적으로 켜져 있지만 변경할 수 있으며 버킷 정책에 원하는 암호화를 사전에 적용할 수도 있습니다

# s3 cors
- 교차 오리진 리소스 공유입니다.
- 오리진은 체계, 즉 프로토콜과 호스트, 즉 도메인 그리고 포트로 구성되어 있습니다
- ![image](https://github.com/user-attachments/assets/bf45fa9b-322d-4b25-83e0-489009e351fe)
- 클라이언트가 S3 버킷에 교차 오리진 요청을 보낼 경우 올바른 CORS 헤더를 활성화해야 합니다
- ![image](https://github.com/user-attachments/assets/22a0fe3e-6119-43ec-8e10-0dcbfed25631)
- CORS는 요청이 다른 오리진에서 발생한 경우 한 S3 버킷에서 이미지나 에셋이나 파일을 검색할 수 있도록 하는 웹 브라우저 보안이라는 점

















