# s3 수명 주기 규칙

아리프 사이클 규칙을 이용해서 클래스간의 객체를 자동으로 옮길 수 있습니다. 
어떤 객체를 다른 클래스로 이전할 최적의 일 수를 결정하는 것은 amazon s3 analytics 를 이용하며 standard 나 standard ia 에 관한 추천사항을 제시합니다. 

*시험 시나리오*

1. 예를 들어 EC2에 애플리케이션이 있고요, 그 앱은 프로필 사진이 Amazon S3에 업로드된 후에
이미지 섬네일을 생성해요, 하지만 그런 섬네일들은 원본 사진으로부터 쉽게 재생성할 수 있어요
그리고 60일 동안만 보관해야 하죠
하지만 원본 이미지는 60일 동안은 곧바로 받을 수 있어야 하고요
그 후에 사용자는 최장 6시간 동안 기다릴 수 있어요
그럼 이런 규칙을 어떻게 설계할까요?

시험 문제가 이렇게 나오죠
그럼 S3 원본 이미지는 Standard 클래스에 있을 수 있고요, 60일 후에 Glacier로 이전하기 위한 라이프사이클 설정이 있어요
그리고 섬네일 이미지는…
예를 들어 이렇게 접두어를 써서 소스와 섬네일을 구분하고요
섬네일은 One-Zone IA에 있을 수 있어요, 왜냐면 빈번히 액세스하는 게 아니고 쉽게 재생성할 수 있으니까요
그리고 60일 후에 그것들을 만료시키거나 삭제하는 라이프사이클 설정이 있을 수 있죠

2. 여러분의 회사 규칙에 따라 30일 동안은 삭제된 S3 객체를 즉각적으로 복구할 수
있어야 해요
비록 그런 경우가 드물긴 하겠지만요, 그 기간이 지나면 최장 365일 동안은 삭제된 객체를 48시간 이내에
복구할 수 있어야 해요

그렇게 하기 위해서 우린 S3 버저닝을 활성화해서 객체 버전을 보관할 수 있고요
삭제된 객체들은 실제로 “삭제 마커”에 의해 감춰져 있죠, 그런 다음에 복구할 수 있을 거예요
다음으로 여러분은 현재 버전이 아닌, 즉 최상위 버전이 아닌 객체들을 Standard IA로 이전하기 위한
규칙을 만들 거예요, 즉 그 현재 버전이 아닌 버전들을 아카이브화를 목적으로
Glacier Deep Archive로 이전할 수 있어요
      
# S3 요청자 지불

사용자가 수많은 대형 파일이 있고 일부 고객이 이를 다운로드하면 요청자 지불 버킷을 활성화 해야 합니다. 
요청자가 객체 데이터 다운로드 비용을 지불합니다. 요청자는 익명이 아니며 aws에서 인증을 받아야 객체에 대한 특정 다운로드를 요청한 요청자에게 청구 가능합니다. 

# s3 이벤트 

이벤트는 객체가 생성되었거나 객체가 삭제되었거나 복구되었거나 복제되는 것을 말합니다. 
이벤트를 필터링 가능하며 sns 토픽이나 sqs queue, 람다 함수등에 알림을 만들고 대상에 전송가능합니다. 
원하는 만큼 s3 이벤트를 만들며 원하는 어떤 타깃에도 전송가능합니다. 이벤트 알림을 작동하려면
iam 권한을 갖고 있어야 합니다. 

s3의 iam 역할을 사용하지 않고 그 대신에 sns 토픽, sqs queue, 혹은 람다 함수에서 리소스 액세스 정책을 정의합니다. 
*sns, sqs, 람다 함수고 이벤트 알림 타깃이라는 점입니다. *
우리의 이벤트는 s3 버킷으로가며 결국 event bridge 로 가게 됩니다. 
때문에 메타데이터 객체 사이즈, 이름으로 필터링을 하고 한꺼번에 다수의 대상에 전송 가능합니다. 

# s3 기준 성능

s3 는 요청이 아주 많을 때 자동으로 확장됩니다. 
접두사당 초당 3,500개의 PUT/COPY/POST/DELETE 초당 5,500개의 GET/HEAD 요청을 지원합니다

멀티 파트 업로드입니다. 100mb 넘는 파일은 멀티파트 업로드를 사용하는 것이 좋습니다. 
5gb가 넘는 파일은 반드시 사용해야합니다. 업로드를 병렬화하므로 전송 속도를 높여 대역폭을 최대화합니다. 

*파일을 여러 작은 청크로 나눕니다. s3에 모든 파트가 업로드되면 자동으로 모든 파트를 합쳐 다시 하나의 큰 파일을 만듭니다.*

업로드와 다운로드를 위한 s3 전송 가속화를 알아보면 파일을 aws 엣지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 s3 버킷으로 전달합니다. 
전송 가속화는 멀티파트 업로드와 같이 사용 가능합니다. 예를 들면 미국의 엣지로케이션을 통해 파일을 업로드하면 퍼플릭인터넷을 이용하여 호주에 있는 s3 버킷으로 빠른 프라이빗 aws 네트워크를 통해 데이터를 전송합니다. 

즉, 퍼플릭 인터넷의 사용량을 최소화하고 프라이빗 aws 네트워크의 사용량을 최대화 하는 겁니다. 

파일을 수신하고 파일을 읽는 가장 효율적인 방법은 s3 바이트 범위 가져오기 기능입니다. 
특정 범위를 가져와서 get요청을 병렬화 합니다. 실패의 경우 복원력이 높아 다운로드 속도를 높일 대 사용합니다. 

*업로드와 다운로드 속도를 높이는 방법과
기준 성능, KMS 제한을 살펴봤습니다*

# s3 select & glacier select

sql문에서 간단히 행과 열을 사용해 필터링 가능하며 네트워크 전송이 줄어들기에 클라이언트 측의 cpu 비용도 줄어듭니다. 
s3 select 는 s3가 대신 파을일 필터링하며 필요함 데이터만 검색합니다. 400% 빠르며 비용도 80% 줄어듭니다. 
csv 파일을 가저오는 부분은 자체 서비스인 서버 측에서 필터링해서 우리에게 필터링된 데이터 세트를 보냅니다. 데이터 크기가 훨씬 작아 저렵해 집니다. 즉 간단한 필터링애 두가지를 추천합니다. 

# s3 batch operations

단일 요청으로 기존 s3객체에서 대량 작업을 수행하는 서비스입니다. 
한 번에 많은 S3 객체의 메타데이터와 프로퍼티를 수정할 수 있고
배치 작업으로 S3 버킷 간에 객체를 복사할 수 있습니다
*S3 버킷 내 암호화되지 않은 모든 객체를 암호화할 수 있습니다*
ACL이나 태그를 수정할 수 있고
S3 Glacier에서 한 번에 많은 객체를 복원할 수 있습니다
Lambda 함수를 호출해 S3 Batch Operations의 모든 객체에서
사용자 지정 작업을 수행할 수도 있습니다


S3 Batch Operations를 사용하면 재시도를 관리할 수 있고
진행 상황을 추적하고 작업 완료 알림을 보내고
보고서 생성 등을 할 수 있습니다

S3 배치에 전달할 객체 목록은 어떻게 생성하는지 궁금하겠죠
S3 Inventory라는 기능을 사용해 객체 목록을 가져오고
S3 Select를 사용해 객체를 필터링합니다
S3 Inventory와 S3 Select를 사용해서
배치 작업에 포함하려는 필터링된 객체 목록을 얻은 다음
S3 Batch Operations에
수행할 작업, 매개 변수와 함께 객체 목록을 전달합니다
그러면 S3 배치가 작업을 수행하고 객체를 처리합니다
주요 사용 사례는
S3 Inventory를 사용해 암호화되지 않은 모든 객체를 찾은 다음
S3 Batch Operations를 사용해 한 번에 모두 암호화하는 겁니다

![image](https://github.com/InHeeS/Certified-AWS/assets/105423951/ef3db1b7-d376-44e7-8adf-cac76c4674a4)


# s3 adv quiz

Question 6:
Amazon RDS PostgreSQL을 사용하여 S3에 파일의 인덱스를 구축하려 합니다. 인덱스 구축을 위해서는 파일 콘텐츠 자체의 메타데이터를 포함하고 있는 S3에 있는 각 객체의 첫 250바이트를 읽는 작업이 필수적입니다. S3 버킷에는 총 50TB에 달하는 100,000개 이상의 파일이 포함되어 있습니다. 이 경우 인덱스를 구축하기 위한 효율적인 방법은 무엇일까요?

answer : s3 버킷을 트래버스하고, 첫 250바이트에 대한 바이트 범위 페치를 발행한 후 , rds에 해당 정보를 저장하는 애플리케이션을 생성 
