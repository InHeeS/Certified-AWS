# s3 moving between storage classes
- 객체를 아카이브할 예정임을 알고 있다면 Glacier 계층이나 Deep Archive 계층으로 이동
- 수명 주기 규칙은 객체를 구성해 다른 스토리지 클래스로 전환하기 위한 전환 작업입니다 예를 들어 생성한지 60일 후에는 표준 클래스로 이동하고 6개월 후에는 아카이빙을 위해 Glacier로 이동할 수 있습니다
- 특정 접두사에 규칙을 지정하여 전체 버킷이나 버킷 내의 특정 경로에 적용되도록 할 수 있음
- 특정 객체 태그에 지정 가능 경리부에만 규칙을 적용 가능
- 지난번에 객체의 클래스를 전환하는 데 가장 적합한 일 수는 Amazon S3 분석을 이용하면 됩니다
  - 표준이나 표준 IA를 권장하며 One Zone-IA나 Glacier에서는 사용할 수 없습니다
- 썸네일은 자주 액세스하지 않고 쉽게 다시 만들 수 있기 때문에 One Zone-IA에 보관할 수 있습니다
- 그리고 수명 주기 설정을 사용해 60일 후 만료되도록 하거나 삭제할 수 있습니다
- 그 다음 최신 버전이 아닌 버전, 즉 최상위 버전이 아닌 버전을 표준 IA로 전환하는 규칙을 만들어 이 최신이 아닌 버전을 추후에 아카이브를 위해 Glacier Deep Archive로 전환할 수 있습니다

# s3 evnet notifications
- 이벤트는 객체가 복원 되거나 복제되는 것
- s3 에서 aws service (sns, sqs, lambda, event bridge) 로 보내기 위해서는 접근 정책이 필요합니다.
- 이벤트는 모두 S3 버킷으로 가서 모두 Amazon EventBridge로 갑니다, 어떤 이벤트인지 상관없이요
- EventBridge에서 이 규칙 덕분에 18 종류의 AWS 서비스를 목적지로 해서 이벤트를 보낼 수 있습니다
  - Step Functions나 Kinesis Streams, Firehose 등에 보낼 수 있죠

# s3 baseline performance, 최적화 방법
- 따라서 버킷의 접두사당 초당 3,500개의 넣기 및 5,500개의 가져오기 규칙을 이해하기 쉽습니다
- 멀티파트 업로드로 100mb 넘는 파일에는 멀티파트 업로드를 사용 (5gb 넘는 파일은 반드시)
  - 그래서 이것은 공공 인터넷의 사용량을 최소화하고 프라이빗 AWS 네트워크의 사용량을 최대화하기 때문에 전송 가속화라고 불립니다
- 효율적으로 읽는 방법은 s3 바이트가 있습니다. 범위 가져오기를 통해 병렬화
- 또는 파일의 일부를 가져올때 업로드 및 다운로드 속도를 높일 수 있습니다.

# s3 user-defined object metadata & s3 object tags
- 객체를 만들고 객체를 업로드할 때 메타데이터도 할당할 수 있습니다
- AWS 내의 특정 태그가 있는 특정 객체에 액세스를 부여할 수 있기 때문입니다
- Amazon S3에서 메타데이터와 태그를 검색할 수 없다는 점으로 메타데이터로 필터링하거나 태그로 필터링할 수 없습니다
- key -value 형태
- S3 버킷에서 검색을 사용하려면 DynamoDB와 같은 데이터베이스에 외부 인덱스를 구축해야 함
  - DynamoDB에서 검색을 수행하면 검색 결과는 Amazon S3의 객체로 추출됩니다
 
# s3 object encryption
- 서버 측 암호화
  - SSE-S3는 Amazon S3 Managed Keys로 암호화입니다
  - KMS 키로 암호화 키를 관리해서 암호화하는 SSE-KMS가 있습니다
  - 사용자 제공 키를 사용하는 SSE-C도 있습니다
- 클라이언트 측 암호화
  - 클라이언트 측에서 암호화한 다음 Amazon S3에 업로드 하는 거죠

- sse-3
  - AWS에서 관리하고 소유합니다, 여러분은 이 키에 접근할 수 없어요
  - 서버 측에서 객체를 암호화 합니다.
  - ![image](https://github.com/user-attachments/assets/90e633d5-eba8-4b0f-8259-6e1516514008)

- sse-kms
  - KMS 서비스를 사용해 자신의 키를 직접 관리하려는 겁니다
  - CloudTrail을 사용해서 사용을 감시할 수 있습니다
  - 사용하려면 이런 헤더가 필요합니다, "x-amz-server-side-encryption":"aws:kms"그러면 서버 측에서 객체가 암호화 됩니다
  - 다른 헤더로 객체를 업로드합니다 헤더에 우리가 사용할 KMS 키를 명시하죠 그러면 객체가 Amazon S3에 나타납니다
  - 그리고 여기에서 사용할 KMS 키는 AWS KMS에서 가지고 오는 겁니다 이 두 가지가 혼합되어 암호화가 됩니다 이 파일이 S3 버킷에 저장되는 거죠
  - S3 버킷에서 파일을 읽으려면 객체 자체에도 접근해야 하지만 이 객체를 암호화하는 데 사용한 KMS 키에도 접근해야 합니다
  - Amazon S3에 파일을 업로드하거나 다운로드 할 때 KMS 키를 활용해야 합니다
  - 그래서 처리량이 아주 많은 S3 버킷이 있다면 KMS 키를 사용하여 모든 것이 암호화되므로 다소 주의가 필요합니다
 
- sse-c
  - 키는 AWS 외부에서 관리되지만 그래도 서버 측 암호화에 포함됩니다
  - AWS에 키를 보내기 때문이죠
  - 하지만 Amazon S3는 여러분이 제공한 키를 절대 저장하지 않아요
  - 작동 방식은, 사용자가 키와 함께 파일을 업로드합니다
  - Amazon S3는 클라이언트가 제공한 키와 객체를 사용해서 암호화를 수행하고 파일을 암호화한 상태로 S3 버킷에 저장합니다
  - 그리고 물론 해당 파일을 읽기 위해서는 사용자가 다시 암호화에 사용한 키를 제공해야 합니다

- client-side encryption
  - s3 에 데이터를 전달하기 전에 클라이언트가 암호화

# encryption in transit (전송 중 암호화)
- 전송 중 암호화는 SSL 혹은 TLS라고 합니다
- SSE-C 유형의 메커니즘을 사용하는 경우 HTTPS 프로토콜 사용이 필수입니다
- 암호화를 강제하는 방법으로 버킷 정책을 사용합니다.

# s3 default encryption
- 기본 암호화인 ㅇsse-s3를 사용합니다.
- 이는 예시에 불과하지만 버킷 정책으로도 버킷의 암호화를 강제로 적용할 수 있다는 점을 확인 가능하다.
- 기본 암호화는 S3에서 기본적으로 켜져 있지만 변경할 수 있으며 버킷 정책에 원하는 암호화를 사전에 적용할 수도 있습니다

# s3 cors
- 교차 오리진 리소스 공유입니다.
- 오리진은 체계, 즉 프로토콜과 호스트, 즉 도메인 그리고 포트로 구성되어 있습니다
- ![image](https://github.com/user-attachments/assets/bf45fa9b-322d-4b25-83e0-489009e351fe)
- 클라이언트가 S3 버킷에 교차 오리진 요청을 보낼 경우 올바른 CORS 헤더를 활성화해야 합니다
- ![image](https://github.com/user-attachments/assets/22a0fe3e-6119-43ec-8e10-0dcbfed25631)
- CORS는 요청이 다른 오리진에서 발생한 경우 한 S3 버킷에서 이미지나 에셋이나 파일을 검색할 수 있도록 하는 웹 브라우저 보안이라는 점

# s3 mfa delete
- 정말로 영구적으로 삭제되지 않도록 하는 데 필요하며 버킷의 버전 관리를 일시 중단하려는 경우에도 사용 가능합니다.
- 먼저 버킷의 버전 관리를 활성화해야 합니다.
- MFA Delete는 특정 객체 버전이 영구적으로 삭제되지 않도록 하는 추가 보호임을 기억해야 합니다

# s3 access logs
- 다시 말해 모든 계정에서 S3 버킷에 보낸 모든 요청이 승인되든 거부되든 다른 S3 버킷에 파일로 기록된다는 뜻입니다
- 그리고 이 데이터는 Amazon Athena와 같은 데이터 분석 도구를 사용하여 분석할 수 있습니다
- 대상 로그 버킷은 동일한 AWS 영역에 있어야 합니다

# pre-signed urls
- 요점은 미리 서명된 URL을 생성하면 해당 URL을 받게 되는 사용자에게 GET이나 PUT 작업에서 생성된 URL의 사용자 권한이 상속된다는 점입니다
- 미리 서명된 URL은 특정 파일을 다운로드하거나 업로드하기 위한 일시적인 액세스가 필요할 때 아주 널리 사용되는 사용 사례입니다
- 파일을 다운로드할 사용자의 목록을 계속 변경되도록 하거나, S3 버킷을 `프라이빗으로 유지`하면서 일시적으로 S3 버킷의 구체적인 위치에 사용자가 파일을 업로드하도록 하는 것이 있습니다

# s3 access points
- 사용자가 많아지고 데이터가 많아질수록 점점 관리할 수 없게 됩니다
- S3 액세스 포인트라고 하는 것을 만들수 있으며, 예를 들어 재무 데이터에 연결할 재무 액세스 포인트를 만들 수 있습니다
- 그리고 이 정책은 S3 버킷 정책과 매우 비슷하고 재무 접두사에 대한 액세스를 승인하고, 읽고, 쓰는 권한을 관리합니다 -> 액세스 포인트 정책을 정의함
- 각 액세스 포인트에는 자체적인 보안이 있습니다
- 프라이빗 트래픽의 오리진이나 VPC로 인터넷에 연결되도록 할 수 있습니다
- 예를 들면 인터넷을 거치지 않는 VPC 액세스의 EC2 인스턴스가 있습니다
VPC 오리진을 거치는 S3 버킷은 VPC 액세스 포인트를 통과합니다
이 VPC 오리진에 대한 액세스 권한을 얻으려면 액세스 포인트에 액세스하려 할 때 VPC 엔드포인트라고 하는 것을
만들어야 합니다

- 다음으로 VPC 엔드포인트에는 정책이 있으며 이 정책은 대상 버킷과 액세스 포인트에 대한 액세스를허용해야 합니다 VPC 엔드포인트 정책을 사용하면 VPC, Amazon S3의 액세스 포인트와 S3 버킷에모두 연결할 수 있습니다

# s3 object lambda
- 수정하기 전 이때 버킷을 복제해 각 객체를 여러 버전으로 만들지 않기 위해 S3 Object Lambda를 사용합니다
- ![image](https://github.com/user-attachments/assets/edd49e2b-5b7a-44f4-b612-c47114e58f9e)

- 보시다시피 S3 버킷은 하나만 필요하고 액세스 포인트와 Object Lambda를 생성해서 데이터를 필요에 따라 수정할 수 있습니다
- XML에서 JSON으로 데이터 형식을 변환하는 경우, 즉시 이미지 크기를 조정하고 워터마크를 추가하는 등의 변환 작업을 하는 경우입니다
- 이 기능의 활용 사례는 PII 데이터 즉 개인 식별 정보를 분석이나 비생산 환경용으로 편집하는 경우
- 특히 워터마크는 객체를 요청하는 사용자별로 지정하기 때문에 S3 Object Lambda의 활용 사례로 유용합니다




